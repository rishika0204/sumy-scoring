import re
import string
import numpy as np
import pandas as pd
import nltk

from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer

# Ensure these NLTK downloads are done somewhere in your environment
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')

def clean_text(text):
    """Replicates your original clean_text logic with a minor optimization."""
    if isinstance(text, float):
        return ""
    text = text.lower()
    # Keep only printable characters
    printable = set(string.printable)
    return "".join(ch for ch in text if ch in printable)

def find_keywords(text):
    """
    Same overall logic as your original code:
    1) Clean text
    2) Tokenize, POS tag, lemmatize
    3) Filter by POS tags & external stopwords
    4) Build adjacency matrix with distance-based weighting
    5) Run PageRank
    6) Build phrases and remove single-word duplicates
    7) Score phrases, sort, and return top 50
    """

    # 1) Clean text
    cleaned_text = clean_text(text)
    tokens = word_tokenize(cleaned_text)
    if not tokens:
        return ""

    # 2) POS tag & lemmatize (same as original)
    pos_tags = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    adjective_tags = ["JJ", "JJR", "JJS"]

    # Lemmatize adjectives with pos='a', else default
    lemmatized_text = [
        lemmatizer.lemmatize(word, "a") if tag in adjective_tags else lemmatizer.lemmatize(word)
        for (word, tag) in pos_tags
    ]

    # Re‚Äêtag after lemmatization (same as original)
    pos_tags_2 = nltk.pos_tag(lemmatized_text)

    # 3) Build stopword list by excluding certain POS tags
    #    (NN, NNS, NNP, NNPS, JJ, JJR, JJS, VBG, FW are *kept*)
    keep_tags = {"NN", "NNS", "NNP", "NNPS", "JJ", "JJR", "JJS", "VBG", "FW"}
    removed_words = [w for (w, t) in pos_tags_2 if t not in keep_tags]

    # Add punctuation & external file-based stopwords
    punctuation = list(string.punctuation)
    additional_stopwords = []
    try:
        with open("long_stopwords.txt", "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    additional_stopwords.append(line)
    except FileNotFoundError:
        pass

    stopwords_plus = set(removed_words + punctuation + additional_stopwords)

    # Filter text
    processed_text = [w for w in lemmatized_text if w not in stopwords_plus]
    if not processed_text:
        return ""

    # 4) Build adjacency matrix with distance-based weighting
    vocabulary = list(set(processed_text))
    vocab_len = len(vocabulary)
    word_index = {word: idx for idx, word in enumerate(vocabulary)}

    weighted_edge = np.zeros((vocab_len, vocab_len), dtype=np.float32)
    covered_cooccurrences = set()

    window_size = 3
    # Slide over processed_text
    for start_i in range(len(processed_text) - window_size + 1):
        # Compare each word in the window with each other
        for j in range(window_size):
            idx_i = word_index[processed_text[start_i + j]]
            for k in range(j + 1, window_size):
                idx_j = word_index[processed_text[start_i + k]]
                if idx_i == idx_j:
                    continue
                dist = abs(j - k)  # distance inside the window
                if dist == 0:
                    dist = 1  # safeguard

                # Only update if not already covered
                pair1 = (idx_i, idx_j)
                pair2 = (idx_j, idx_i)
                if pair1 not in covered_cooccurrences:
                    weighted_edge[idx_i][idx_j] += 1.0 / dist
                    covered_cooccurrences.add(pair1)
                if pair2 not in covered_cooccurrences:
                    weighted_edge[idx_j][idx_i] += 1.0 / dist
                    covered_cooccurrences.add(pair2)

    # 5) Run PageRank exactly as in your code
    score = np.ones(vocab_len, dtype=np.float32)
    inout = np.sum(weighted_edge, axis=1)  # or axis=1 if that was your original

    damping_factor = 0.85
    max_iterations = 50
    threshold = 0.0001

    for _ in range(max_iterations):
        prev_score = np.copy(score)
        for i in range(vocab_len):
            summation = 0.0
            for j in range(vocab_len):
                if weighted_edge[i][j] != 0:
                    # Weighted in-degree from j
                    summation += (weighted_edge[i][j] / inout[j]) * prev_score[j]
            score[i] = (1 - damping_factor) + damping_factor * summation

        if np.sum(np.abs(prev_score - score)) <= threshold:
            break

    # 6) Build phrases (same logic of splitting on stopwords)
    phrases = []
    current_phrase = []
    stopwords_set = stopwords_plus  # just a name shortcut

    for w in lemmatized_text:
        if w in stopwords_set:
            if current_phrase:
                phrases.append(current_phrase)
                current_phrase = []
        else:
            current_phrase.append(w)
    if current_phrase:
        phrases.append(current_phrase)

    # Remove duplicates
    unique_phrases = []
    seen = set()
    for ph in phrases:
        tup = tuple(ph)
        if tup not in seen:
            seen.add(tup)
            unique_phrases.append(ph)

    # Remove single-word phrases if they appear in multi-word phrases
    for w in vocabulary:
        single = [w]
        if single in unique_phrases:
            # If w also appears in any multi-word phrase, remove it
            for ph in unique_phrases:
                if len(ph) > 1 and w in ph:
                    unique_phrases.remove(single)
                    break

    # 7) Score phrases and sort
    phrase_scores = []
    keywords = []
    for ph in unique_phrases:
        # Sum the PageRank scores of each word in the phrase
        p_score = sum(score[word_index[w]] for w in ph if w in word_index)
        phrase_scores.append(p_score)
        keywords.append(" ".join(ph))

    sorted_idx = np.argsort(phrase_scores)[::-1]  # descending
    keywords_num = min(50, len(sorted_idx))
    top_keywords = [keywords[i] for i in sorted_idx[:keywords_num]]

    return ", ".join(top_keywords)

def main():
    # Example usage
    df = pd.read_excel(r"C:/Users/2025344/Demo1/issues_data.xlsx")
    column_data = df.iloc[:, 13]  # or whichever column
    combined_text = " ".join(column_data.dropna().astype(str).tolist())

    # 80/20 split
    split_index = int(len(combined_text) * 0.8)
    train_text = combined_text[:split_index]
    test_text = combined_text[split_index:]

    # Extract keywords from the training portion
    keywords = find_keywords(train_text)
    print("Top keywords/phrases:")
    print(keywords)

if __name__ == "__main__":
    main()
