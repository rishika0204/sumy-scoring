# keywords_extraction.py
import re
import string
import time
import numpy as np
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from scipy.sparse import lil_matrix
from concurrent.futures import ThreadPoolExecutor


def clean_text(text):
    """
    Clean text by converting to lowercase and removing non-printable characters.
    """
    if not isinstance(text, str):
        return ""
    text = text.lower()
    printable = set(string.printable)
    return ''.join(ch for ch in text if ch in printable)


def build_partial_matrix(tokens, word_index, window_size):
    """
    Build a sparse co-occurrence matrix for a chunk of tokens using a sliding window.
    """
    vocab_len = len(word_index)
    matrix = lil_matrix((vocab_len, vocab_len), dtype=np.float32)

    # Slide window over tokens
    for i in range(len(tokens) - window_size + 1):
        window = tokens[i : i + window_size]
        for j in range(len(window)):
            idx1 = word_index.get(window[j])
            if idx1 is None:
                continue
            for k in range(j + 1, len(window)):
                idx2 = word_index.get(window[k])
                if idx2 is None or idx1 == idx2:
                    continue
                dist = abs(j - k) or 1
                matrix[idx1, idx2] += 1.0 / dist
                matrix[idx2, idx1] += 1.0 / dist
    return matrix


def find_keywords(text, number_of_keywords=5):
    """
    Extract top keywords from text using a custom TextRank-style algorithm.

    Args:
        text (str): Input text to process.
        number_of_keywords (int): Number of keywords/phrases to return.

    Returns:
        List[str]: Top keywords or phrases.
    """
    start_time = time.time()

    # 1) Clean and tokenize
    cleaned = clean_text(text)
    tokens = word_tokenize(cleaned)
    if not tokens:
        return []

    # 2) POS tagging and lemmatization
    pos_tags = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    adj_tags = {"JJ", "JJR", "JJS"}
    lemmas = [lemmatizer.lemmatize(w, 'a') if t in adj_tags else lemmatizer.lemmatize(w)
              for w, t in pos_tags]

    # 3) Filter by POS
    pos2 = nltk.pos_tag(lemmas)
    keep_tags = {"NN", "NNS", "NNP", "NNPS", "JJ", "JJR", "JJS", "VBG", "FW"}
    removed = [w for w, t in pos2 if t not in keep_tags]

    # 4) Stopwords and punctuation
    punctuation = set(string.punctuation)
    extra_stops = []
    try:
        with open('long_stopwords.txt') as f:
            extra_stops = [ln.strip() for ln in f if ln.strip()]
    except FileNotFoundError:
        pass
    stopset = set(removed) | punctuation | set(extra_stops)

    processed = [w for w in lemmas if w not in stopset]
    if not processed:
        return []

    # 5) Build graph
    vocab = list(set(processed))
    word_index = {w: i for i, w in enumerate(vocab)}
    vocab_len = len(vocab)
    window_size = 3

    # split for parallel processing
    num_threads = 8
    chunk_size = max(1, len(processed) // num_threads)
    chunks = [processed[i:i+chunk_size] for i in range(0, len(processed), chunk_size)]

    matrix_start = time.time()
    with ThreadPoolExecutor(max_workers=num_threads) as exec:
        parts = list(exec.map(lambda c: build_partial_matrix(c, word_index, window_size), chunks))
    matrix_end = time.time()

    # combine and normalize
    weighted = sum(parts).tocsr()
    out = np.array(weighted.sum(axis=1)).flatten()
    out[out == 0] = 1e-9
    T_csc = weighted.tocsc()
    for j in range(vocab_len):
        s, e = T_csc.indptr[j], T_csc.indptr[j+1]
        T_csc.data[s:e] /= out[j]
    T = T_csc.tocsr()

    # 6) PageRank
    d = 0.85
    iters = 50
    scores = np.ones(vocab_len, dtype=np.float32)
    for _ in range(iters):
        scores = (1 - d) + d * T.dot(scores)

    # 7) Build candidate phrases (bigrams)
    phrases = [processed[i:i+2] for i in range(len(processed) - 1)]
    unique_phrases = []
    seen = set()
    for ph in phrases:
        key = tuple(ph)
        if key not in seen:
            seen.add(key)
            unique_phrases.append(ph)

    # 8) Score and select
    phrase_scores = []
    candidates = []
    for ph in unique_phrases:
        sc = sum(scores[word_index[w]] for w in ph if w in word_index)
        phrase_scores.append(sc)
        candidates.append(" ".join(ph))
    if not phrase_scores:
        return []

    order = np.argsort(phrase_scores)[::-1]
    top_n = min(number_of_keywords, len(order))
    result = [candidates[i] for i in order[:top_n]]

    end_time = time.time()
    print(f"Keyword extraction done in {(end_time - start_time):.2f}s (matrix build: {(matrix_end - matrix_start):.2f}s)")
    return result


# main.py
if __name__ == '__main__':
    import pandas as pd
    from keywords_extraction import find_keywords

    def main():
        input_file = 'path/to/your/input.xlsx'
        output_file = 'path/to/your/output_with_keywords.xlsx'

        # 1) load data
        df = pd.read_excel(input_file)
        texts = df.iloc[:, 1]

        # 2) extract keywords
        for idx, txt in texts.items():
            print(f"Processing row: {idx + 1}")
            kws = find_keywords(txt, number_of_keywords=7)
            df.at[idx, 'Keywords'] = ', '.join(kws)

        # 3) save results
        df.to_excel(output_file, index=False)
        print(f"Final excel file with keywords created at: {output_file}")

    main()
