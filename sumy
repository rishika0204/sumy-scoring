"""
keyword_extraction.py
Example streamlined TextRank-based keyword extraction in one file.
"""

import nltk
import numpy as np
import pandas as pd
import string
import math
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from collections import defaultdict

# -------------------------------------------------------------------
# 1) Global stopwords read only once
# -------------------------------------------------------------------
with open("long_stopwords.txt", "r", encoding="utf-8") as f:
    BASE_STOPWORDS = {line.strip() for line in f}

# If you want to add custom stopwords, put them here:
ADDITIONAL_STOPWORDS = set()

# Combine them:
STOPWORDS = BASE_STOPWORDS.union(ADDITIONAL_STOPWORDS)

# We'll also define a punctuation set once:
PUNCTUATIONS = set(string.punctuation)

# -------------------------------------------------------------------
# 2) Helpers for POS -> WordNet POS, cleaning, tokenizing, etc.
# -------------------------------------------------------------------
def nltk_tag_to_wordnet_pos(tag):
    """Convert NLTK's POS tags to WordNet's POS tags for lemmatizer."""
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # fallback if it's something else

def clean_text(text):
    """Lowercase text, remove non-printable chars, handle floats."""
    if isinstance(text, float):
        text = ""
    text = text.lower()
    printable = set(string.printable)
    return "".join(ch for ch in text if ch in printable)

def tokenize_and_lemmatize(text):
    """Tokenize, POS-tag, and lemmatize in one pass, skipping stopwords/punct."""
    tokens = nltk.word_tokenize(text)
    tagged = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = []
    for (word, tag) in tagged:
        if word not in STOPWORDS and word not in PUNCTUATIONS:
            wn_pos = nltk_tag_to_wordnet_pos(tag)
            lemma = lemmatizer.lemmatize(word, pos=wn_pos)
            lemmatized_tokens.append(lemma)
    return lemmatized_tokens

# -------------------------------------------------------------------
# 3) Build adjacency list (sparse graph) instead of dense NxN matrix
# -------------------------------------------------------------------
def build_adjacency_list(tokens, window_size=3):
    """
    Returns adjacency dict: adjacency[w1][w2] = weight
    Also returns a list of unique vocab items (vocab) and dict mapping (vocab_dict).
    """
    adjacency = defaultdict(lambda: defaultdict(float))
    covered = set()

    # For clarity, let's also keep an explicit "vocab" list:
    vocab = list(set(tokens))
    vocab_dict = {w: i for i, w in enumerate(vocab)}

    for start in range(len(tokens) - window_size + 1):
        window = tokens[start : start + window_size]
        for i in range(len(window)):
            for j in range(len(window)):
                if i != j:
                    w_i = window[i]
                    w_j = window[j]
                    # Avoid double-counting the exact same positions
                    if (w_i, w_j, start+i, start+j) not in covered:
                        # Weighted by inverse distance in the window
                        adjacency[w_i][w_j] += 1.0 / abs(i - j)
                        covered.add((w_i, w_j, start+i, start+j))

    return adjacency, vocab, vocab_dict

# -------------------------------------------------------------------
# 4) TextRank scoring with power iteration on adjacency list
# -------------------------------------------------------------------
def text_rank_scores(adjacency, vocab, damping=0.85, max_iter=50, tol=1e-4):
    """
    adjacency: dict of dict (sparse adjacency list)
    vocab: list of all unique tokens
    Returns a NumPy array of final scores aligned with 'vocab'.
    """
    n = len(vocab)
    if n == 0:
        return np.array([])
    vocab_dict = {w: i for i, w in enumerate(vocab)}

    # Initialize scores uniformly
    scores = np.ones(n, dtype=float) / n

    # Precompute out-degrees for each node
    out_degree = {}
    for w_i in adjacency:
        out_degree[w_i] = sum(adjacency[w_i].values())

    for _ in range(max_iter):
        prev_scores = scores.copy()
        # Update each word's score
        for w_i in adjacency:
            i = vocab_dict[w_i]
            s = 0.0
            for w_j, weight in adjacency[w_i].items():
                j = vocab_dict[w_j]
                s += (weight / out_degree[w_j]) * prev_scores[j]
            scores[i] = (1 - damping) + damping * s

        # Check convergence
        if np.abs(prev_scores - scores).sum() < tol:
            break

    return scores

# -------------------------------------------------------------------
# 5) Phrase extraction (split on stopwords or punctuation)
# -------------------------------------------------------------------
def extract_phrases(tokens):
    """
    Splits tokens into "phrases" by stopwords/punct.
    Because we already removed stopwords in 'tokenize_and_lemmatize',
    you can skip them or adapt to your needs.
    """
    phrases = []
    phrase = []

    # Since we've already stripped out STOPWORDS and punctuation,
    # each token is presumably "valid". But let's illustrate logic:
    for w in tokens:
        # If you *hadn't* removed them, you'd check:
        # if w in STOPWORDS or w in PUNCTUATIONS:
        #     if phrase:
        #         phrases.append(phrase)
        #     phrase = []
        # else:
        phrase.append(w)

    if phrase:
        phrases.append(phrase)

    # De-duplicate
    unique_phrases = []
    seen = set()
    for p in phrases:
        p_tuple = tuple(p)
        if p_tuple not in seen:
            seen.add(p_tuple)
            unique_phrases.append(p)

    return unique_phrases

# -------------------------------------------------------------------
# 6) Main function to find keywords for one row
# -------------------------------------------------------------------
def find_keywords(text, row_index, df, top_k=5):
    """
    Cleans text, lemmatizes, builds adjacency list, does TextRank,
    extracts top-k phrases, and writes them to DataFrame at row 'row_index'.
    """
    cleaned = clean_text(text)
    tokens = tokenize_and_lemmatize(cleaned)

    if not tokens:
        df.at[row_index, "Keywords using custom TextRank Algorithm"] = ""
        return

    # Build graph
    adjacency, vocab, vocab_dict = build_adjacency_list(tokens, window_size=3)
    # Compute scores
    scores = text_rank_scores(adjacency, vocab)
    if scores.size == 0:
        df.at[row_index, "Keywords using custom TextRank Algorithm"] = ""
        return

    # Map each word -> final score
    word2score = {}
    for i, w in enumerate(vocab):
        word2score[w] = scores[i]

    # Extract phrases, then compute phrase-level scores
    phrases = extract_phrases(tokens)
    phrase_scores = []
    phrase_strings = []
    for p in phrases:
        sc = sum(word2score.get(w, 0.0) for w in p)
        phrase_scores.append(sc)
        phrase_strings.append(" ".join(p))

    # If no phrases, done
    if not phrase_scores:
        df.at[row_index, "Keywords using custom TextRank Algorithm"] = ""
        return

    # Convert to NumPy for partial sorting
    phrase_scores = np.array(phrase_scores)
    k = min(top_k, len(phrase_scores))

    # Grab top-k indices (argpartition is faster than full sort)
    idx = np.argpartition(phrase_scores, -k)[-k:]
    # Sort just those top-k in descending order
    idx_sorted = idx[np.argsort(phrase_scores[idx])[::-1]]

    # Get the top-k phrase strings
    top_phrases = [phrase_strings[i] for i in idx_sorted]

    # Join them up and store in the DataFrame
    df.at[row_index, "Keywords using custom TextRank Algorithm"] = ", ".join(top_phrases)

# -------------------------------------------------------------------
# 7) Process entire Excel file at once
# -------------------------------------------------------------------
def extract_keywords_from_excel(file_path):
    """
    Reads Excel, processes each row for keywords, writes back once at the end.
    """
    df = pd.read_excel(file_path)
    # Assuming the first column in your Excel is the text to process:
    column_data = df.iloc[:, 0]

    # For each row, run find_keywords
    for i, text in column_data.iteritems():
        find_keywords(text, i, df, top_k=5)

    # Write to Excel only once
    df.to_excel(file_path, index=False)
