import pandas as pd
import re
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from nltk.corpus import stopwords
import nltk

# Make sure you have downloaded the NLTK stopwords if needed
# nltk.download('stopwords')

# ------------------------------------------------------------------
# 1. Load the data
# ------------------------------------------------------------------
# Adjust file paths and columns as needed
train_df = pd.read_csv('train.tsv', sep='\t', header=None, names=['text', 'label'])
dev_df   = pd.read_csv('dev.tsv',   sep='\t', header=None, names=['text', 'label'])

# If your test set has labels
test_df  = pd.read_csv('test.csv', sep=',', header=None, names=['text', 'label'])

# If your test set does NOT have labels, do this instead:
# test_df  = pd.read_csv('test.csv', sep=',', header=None, names=['text'])

# ------------------------------------------------------------------
# 2. Basic text preprocessing function
# ------------------------------------------------------------------
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation (optional; sometimes keeping punctuation can help)
    text = re.sub(r'[^\w\s]', '', text)
    # Strip extra whitespace
    text = text.strip()
    return text

# Apply preprocessing to all datasets
train_df['text'] = train_df['text'].apply(preprocess_text)
dev_df['text']   = dev_df['text'].apply(preprocess_text)
test_df['text']  = test_df['text'].apply(preprocess_text)

# ------------------------------------------------------------------
# 3. Define a Pipeline (Tfidf + Naive Bayes)
# ------------------------------------------------------------------
pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),
    ('classifier', MultinomialNB())
])

# ------------------------------------------------------------------
# 4. Hyperparameter Tuning with GridSearchCV
# ------------------------------------------------------------------
# We will search over a few common parameters:
# - ngram_range: (1,1) = unigrams only, (1,2) = unigrams + bigrams
# - min_df: minimum document frequency for a token
# - alpha: smoothing parameter for Naive Bayes
param_grid = {
    'vectorizer__ngram_range': [(1, 1), (1, 2)],
    'vectorizer__min_df': [1, 2, 5],
    'classifier__alpha': [0.1, 1, 5]
}

grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=3,               # 3-fold cross-validation
    scoring='accuracy', # or 'f1_macro' if class imbalance is high
    verbose=1,
    n_jobs=-1           # Use all available cores for speed
)

# ------------------------------------------------------------------
# 5. Fit on training data, evaluate on dev set
# ------------------------------------------------------------------
grid_search.fit(train_df['text'], train_df['label'])

print("\nBest Hyperparameters found via GridSearch:")
print(grid_search.best_params_)

best_model = grid_search.best_estimator_

# Evaluate on dev set
dev_preds = best_model.predict(dev_df['text'])
print("\nDevelopment Set Evaluation:")
print(classification_report(dev_df['label'], dev_preds))
print("Dev Accuracy:", accuracy_score(dev_df['label'], dev_preds))

# ------------------------------------------------------------------
# 6. (Optional) Retrain on combined (train + dev) for final model
# ------------------------------------------------------------------
# If you want to squeeze out maximum performance on the test set,
# you can combine train + dev, then refit the best_model's pipeline.
train_dev_df = pd.concat([train_df, dev_df], axis=0)

best_model.fit(train_dev_df['text'], train_dev_df['label'])

# ------------------------------------------------------------------
# 7. Test Set Predictions
# ------------------------------------------------------------------
if 'label' in test_df.columns:
    # If your test set has labels, evaluate performance
    test_preds = best_model.predict(test_df['text'])
    print("\nTest Set Evaluation:")
    print(classification_report(test_df['label'], test_preds))
    print("Test Accuracy:", accuracy_score(test_df['label'], test_preds))
else:
    # If test set has no labels, just predict and possibly save
    test_preds = best_model.predict(test_df['text'])
    print("\nPredictions on Test Set (no labels available):")
    for i, pred in enumerate(test_preds[:10]):
        print(f"Text: {test_df['text'][i]} --> Predicted Label: {pred}")
    # Optionally save all predictions to file
    # pd.DataFrame({'text': test_df['text'], 'pred_label': test_preds}).to_csv('test_predictions.csv', index=False)
