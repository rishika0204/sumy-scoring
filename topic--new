import re
import string
import numpy as np
import pandas as pd
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from scipy.sparse import lil_matrix, csr_matrix

# For LDA (and optionally NMF)
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF

# Ensure these NLTK resources are available
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

def clean_text(text):
    """Clean text similarly to your original implementation."""
    if isinstance(text, float):
        return ""
    text = text.lower()
    printable = set(string.printable)
    return "".join(ch for ch in text if ch in printable)

def find_keywords(text):
    """
    Extract keywords using your PageRank-based approach over a sparse co-occurrence matrix.
    This version also removes consecutive duplicate words within a phrase.
    """
    # --- 1. Clean and tokenize ---
    cleaned_text = clean_text(text)
    tokens = word_tokenize(cleaned_text)
    if not tokens:
        return ""
    
    # --- 2. POS tagging and lemmatization ---
    pos_tags = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    adjective_tags = {"JJ", "JJR", "JJS"}
    lemmatized_text = [
        lemmatizer.lemmatize(word, "a") if tag in adjective_tags else lemmatizer.lemmatize(word)
        for word, tag in pos_tags
    ]
    pos_tags_2 = nltk.pos_tag(lemmatized_text)
    
    # Keep only certain POS tags
    keep_tags = {"NN", "NNS", "NNP", "NNPS", "JJ", "JJR", "JJS", "VBG", "FW"}
    removed_words = [w for (w, t) in pos_tags_2 if t not in keep_tags]
    
    # Remove punctuation and optional external stopwords
    punctuation = list(string.punctuation)
    additional_stopwords = []
    try:
        with open("long_stopwords.txt", "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    additional_stopwords.append(line)
    except FileNotFoundError:
        pass
    stopwords_plus = set(removed_words + punctuation + additional_stopwords)
    
    # Filter out the stopwords
    processed_text = [w for w in lemmatized_text if w not in stopwords_plus]
    if not processed_text:
        return ""
    
    # --- 3. Build the sparse co-occurrence matrix ---
    vocabulary = list(set(processed_text))
    vocab_len = len(vocabulary)
    word_index = {word: idx for idx, word in enumerate(vocabulary)}
    window_size = 3
    weighted_edge = lil_matrix((vocab_len, vocab_len), dtype=np.float32)
    
    for start_i in range(len(processed_text) - window_size + 1):
        window_words = processed_text[start_i : start_i + window_size]
        for j in range(window_size):
            idx_i = word_index[window_words[j]]
            for k in range(j + 1, window_size):
                idx_j = word_index[window_words[k]]
                if idx_i == idx_j:
                    continue
                dist = abs(j - k) or 1
                weighted_edge[idx_i, idx_j] += 1.0 / dist
                weighted_edge[idx_j, idx_i] += 1.0 / dist

    # --- 4. Compute PageRank ---
    weighted_edge = weighted_edge.tocsr()
    inout = np.array(weighted_edge.sum(axis=1)).flatten()
    inout[inout == 0] = 1e-9  # prevent division by zero
    
    # Normalize columns to build transition matrix T
    T_csc = weighted_edge.tocsc()
    for j in range(vocab_len):
        start_ptr, end_ptr = T_csc.indptr[j], T_csc.indptr[j+1]
        if inout[j] != 0:
            T_csc.data[start_ptr:end_ptr] /= inout[j]
    T = T_csc.tocsr()
    
    damping_factor = 0.85
    max_iterations = 50
    threshold = 1e-4
    score = np.ones(vocab_len, dtype=np.float32)
    
    for _ in range(max_iterations):
        prev_score = score.copy()
        score = (1 - damping_factor) + damping_factor * (T.dot(prev_score))
        if np.sum(np.abs(prev_score - score)) <= threshold:
            break

    # --- 5. Build phrases ---
    phrases = []
    current_phrase = []
    for w in lemmatized_text:
        if w in stopwords_plus:
            if current_phrase:
                phrases.append(current_phrase)
                current_phrase = []
        else:
            current_phrase.append(w)
    if current_phrase:
        phrases.append(current_phrase)
    
    # Remove duplicate phrases while preserving order
    unique_phrases = []
    seen = set()
    for ph in phrases:
        tup = tuple(ph)
        if tup not in seen:
            seen.add(tup)
            unique_phrases.append(ph)
    
    # Optional: remove single-word phrases if they appear within any multi-word phrase
    for w in vocabulary:
        single = [w]
        if single in unique_phrases:
            for ph in unique_phrases:
                if len(ph) > 1 and w in ph:
                    unique_phrases.remove(single)
                    break
    
    # Remove consecutive duplicate words (e.g., "client client investment" -> "client investment")
    deduped_phrases = []
    for ph in unique_phrases:
        if not ph:
            continue
        cleaned = [ph[0]]
        for i in range(1, len(ph)):
            if ph[i] != ph[i - 1]:
                cleaned.append(ph[i])
        deduped_phrases.append(cleaned)
    
    # --- 6. Score phrases and select top keywords ---
    phrase_scores = []
    keywords_list = []
    for ph in deduped_phrases:
        p_score = sum(score[word_index[w]] for w in ph if w in word_index)
        phrase_scores.append(p_score)
        keywords_list.append(" ".join(ph))
    
    sorted_idx = np.argsort(phrase_scores)[::-1]
    keywords_num = min(50, len(sorted_idx))
    top_keywords = [keywords_list[i] for i in sorted_idx[:keywords_num]]
    
    return ", ".join(top_keywords)

def topic_modeling_with_keywords_lda(documents, n_topics=8, top_n_docs=10):
    """
    Fit an LDA model on the corpus. For each topic:
      1. Sort documents by their probability for that topic.
      2. Select the top N documents.
      3. Aggregate their text into one string.
      4. Run the PageRank-based keyword extraction (find_keywords) on that aggregated text.
    The output uses the keywords from find_keywords as the topic terms.
    """
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.decomposition import LatentDirichletAllocation

    print("\n=== LDA Topic Modeling with PageRank-based Topic Terms ===")
    
    # Convert documents to a bag-of-words representation
    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)
    doc_term_matrix = vectorizer.fit_transform(documents)
    
    # Fit LDA model
    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda_model.fit(doc_term_matrix)
    
    # Get topic probabilities for each document
    doc_topic_probs = lda_model.transform(doc_term_matrix)
    
    for topic_idx in range(n_topics):
        # Sort documents by their probability for this topic (descending order)
        sorted_docs = np.argsort(doc_topic_probs[:, topic_idx])[::-1]
        top_doc_indices = sorted_docs[:top_n_docs]
        
        # Aggregate the text of the top documents for this topic
        topic_text = " ".join([documents[i] for i in top_doc_indices])
        
        # Use your PageRank-based keyword extraction on the aggregated topic text
        topic_keywords = find_keywords(topic_text)
        
        print(f"\nTopic {topic_idx+1} (PageRank-based Keywords):")
        print(topic_keywords)

# Optional: you can also create a similar function for NMF if needed.

def main():
    # Load your data from an Excel file (adjust the file path and column index as needed)
    df = pd.read_excel(r"C:/Users/2025344/Demo1/issues_data.xlsx")
    # Assume that the text is in the 14th column (index 13)
    column_data = df.iloc[:, 13]
    
    # Convert each row to a string and drop missing values
    documents = column_data.dropna().astype(str).tolist()
    
    # 1. Run the PageRank-based keyword extraction on the aggregated text (for reference)
    combined_text = " ".join(documents)
    overall_keywords = find_keywords(combined_text)
    print("Overall PageRank-based Keywords:")
    print(overall_keywords)
    
    # 2. Run LDA-based topic modeling and use find_keywords for topic terms
    topic_modeling_with_keywords_lda(documents, n_topics=8, top_n_docs=10)

if __name__ == "__main__":
    main()
