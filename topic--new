import re
import string
import numpy as np
import pandas as pd
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from scipy.sparse import lil_matrix, csr_matrix

# For LDA and NMF
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF

# Ensure these NLTK resources are available
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

def clean_text(text):
    """Clean text similarly to your original approach."""
    if isinstance(text, float):
        return ""
    text = text.lower()
    printable = set(string.printable)
    return "".join(ch for ch in text if ch in printable)

def find_keywords(text):
    """
    PageRank-based keyword extraction over a sparse co-occurrence matrix,
    with extra logic to remove consecutive duplicate words in a phrase.
    """
    # --- 1. Clean and tokenize ---
    cleaned_text = clean_text(text)
    tokens = word_tokenize(cleaned_text)
    if not tokens:
        return ""
    
    # --- 2. POS tagging and lemmatization ---
    pos_tags = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    adjective_tags = {"JJ", "JJR", "JJS"}
    lemmatized_text = [
        lemmatizer.lemmatize(word, "a") if tag in adjective_tags else lemmatizer.lemmatize(word)
        for word, tag in pos_tags
    ]
    pos_tags_2 = nltk.pos_tag(lemmatized_text)
    
    # Build a stopword list based on POS filtering (keep only certain tags)
    keep_tags = {"NN", "NNS", "NNP", "NNPS", "JJ", "JJR", "JJS", "VBG", "FW"}
    removed_words = [w for (w, t) in pos_tags_2 if t not in keep_tags]
    
    # Also remove punctuation and any optional external stopwords
    punctuation = list(string.punctuation)
    additional_stopwords = []
    try:
        with open("long_stopwords.txt", "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    additional_stopwords.append(line)
    except FileNotFoundError:
        pass
    
    stopwords_plus = set(removed_words + punctuation + additional_stopwords)
    
    # Filter out these stopwords from the text
    processed_text = [w for w in lemmatized_text if w not in stopwords_plus]
    if not processed_text:
        return ""
    
    # --- 3. Build the sparse co-occurrence matrix ---
    vocabulary = list(set(processed_text))
    vocab_len = len(vocabulary)
    word_index = {word: idx for idx, word in enumerate(vocabulary)}
    window_size = 3
    
    weighted_edge = lil_matrix((vocab_len, vocab_len), dtype=np.float32)
    
    for start_i in range(len(processed_text) - window_size + 1):
        window_words = processed_text[start_i : start_i + window_size]
        for j in range(window_size):
            idx_i = word_index[window_words[j]]
            for k in range(j + 1, window_size):
                idx_j = word_index[window_words[k]]
                if idx_i == idx_j:
                    continue
                dist = abs(j - k) or 1
                weighted_edge[idx_i, idx_j] += 1.0 / dist
                weighted_edge[idx_j, idx_i] += 1.0 / dist

    # --- 4. PageRank Computation ---
    weighted_edge = weighted_edge.tocsr()
    
    inout = np.array(weighted_edge.sum(axis=1)).flatten()
    inout[inout == 0] = 1e-9  # prevent division by zero

    # Normalize columns to build transition matrix T
    T_csc = weighted_edge.tocsc()
    for j in range(vocab_len):
        start_ptr, end_ptr = T_csc.indptr[j], T_csc.indptr[j+1]
        if inout[j] != 0:
            T_csc.data[start_ptr:end_ptr] /= inout[j]
    T = T_csc.tocsr()
    
    damping_factor = 0.85
    max_iterations = 50
    threshold = 1e-4
    score = np.ones(vocab_len, dtype=np.float32)
    
    for _ in range(max_iterations):
        prev_score = score.copy()
        score = (1 - damping_factor) + damping_factor * (T.dot(prev_score))
        if np.sum(np.abs(prev_score - score)) <= threshold:
            break

    # --- 5. Build phrases ---
    phrases = []
    current_phrase = []
    for w in lemmatized_text:
        if w in stopwords_plus:
            if current_phrase:
                phrases.append(current_phrase)
                current_phrase = []
        else:
            current_phrase.append(w)
    if current_phrase:
        phrases.append(current_phrase)
    
    # Remove duplicate phrases while preserving order
    unique_phrases = []
    seen = set()
    for ph in phrases:
        tup = tuple(ph)
        if tup not in seen:
            seen.add(tup)
            unique_phrases.append(ph)
    
    # Optional: remove single-word phrases if they appear in multi-word phrases
    for w in vocabulary:
        single = [w]
        if single in unique_phrases:
            for ph in unique_phrases:
                if len(ph) > 1 and w in ph:
                    unique_phrases.remove(single)
                    break

    # **Remove consecutive duplicates** in each phrase,
    # turning ["client", "client", "investment"] -> ["client", "investment"]
    deduped_phrases = []
    for ph in unique_phrases:
        if not ph:
            continue
        cleaned = [ph[0]]
        for i in range(1, len(ph)):
            if ph[i] != ph[i - 1]:
                cleaned.append(ph[i])
        deduped_phrases.append(cleaned)
    
    # --- 6. Score phrases and select the top keywords ---
    phrase_scores = []
    keywords_list = []
    for ph in deduped_phrases:
        p_score = sum(score[word_index[w]] for w in ph if w in word_index)
        phrase_scores.append(p_score)
        keywords_list.append(" ".join(ph))
    
    sorted_idx = np.argsort(phrase_scores)[::-1]
    keywords_num = min(50, len(sorted_idx))
    top_keywords = [keywords_list[i] for i in sorted_idx[:keywords_num]]
    
    return ", ".join(top_keywords)

def topic_modeling_lda(documents, n_topics=8, n_top_words=10):
    """
    LDA topic modeling with phrase-level (n-gram) features.
    """
    # Use n-grams up to size 2 or 3 to get short phrases
    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2,
                                 ngram_range=(1, 2))  # (1,2) = unigrams + bigrams
    doc_term_matrix = vectorizer.fit_transform(documents)
    
    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda_model.fit(doc_term_matrix)
    
    feature_names = vectorizer.get_feature_names_out()
    
    print("\nLDA Topics (using n-grams):")
    for topic_idx, topic in enumerate(lda_model.components_):
        # Sort by descending weight
        top_indices = topic.argsort()[::-1][:n_top_words]
        top_features = [feature_names[i] for i in top_indices]
        print(f"Topic {topic_idx+1}: {', '.join(top_features)}")

def topic_modeling_nmf(documents, n_topics=8, n_top_words=10):
    """
    NMF topic modeling with phrase-level (n-gram) features.
    """
    # Use TF-IDF with bigrams
    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2,
                                 ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(documents)
    
    nmf_model = NMF(n_components=n_topics, random_state=42)
    nmf_model.fit(tfidf_matrix)
    
    feature_names = vectorizer.get_feature_names_out()
    
    print("\nNMF Topics (using n-grams):")
    for topic_idx, topic in enumerate(nmf_model.components_):
        # Sort by descending weight
        top_indices = topic.argsort()[::-1][:n_top_words]
        top_features = [feature_names[i] for i in top_indices]
        print(f"Topic {topic_idx+1}: {', '.join(top_features)}")

def main():
    # Load your data; adjust the file path and column index
    df = pd.read_excel(r"C:/Users/2025344/Demo1/issues_data.xlsx")
    column_data = df.iloc[:, 13]
    
    # Convert each row to a string for topic modeling
    documents = column_data.dropna().astype(str).tolist()
    
    # 1. PageRank-based keywords on aggregated text
    combined_text = " ".join(documents)
    # If you want an 80% portion of that text, do:
    split_index = int(len(combined_text) * 0.8)
    train_text = combined_text[:split_index]
    keywords = find_keywords(train_text)
    print("Top keywords/phrases (PageRank-based):")
    print(keywords)
    
    # 2. Topic modeling with LDA and n-grams
    topic_modeling_lda(documents, n_topics=8, n_top_words=10)
    
    # 3. Topic modeling with NMF and n-grams
    topic_modeling_nmf(documents, n_topics=8, n_top_words=10)

if __name__ == "__main__":
    main()
