import re
import string
import numpy as np
import pandas as pd
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from scipy.sparse import lil_matrix, csr_matrix

# For LDA and NMF
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF

# Ensure these NLTK resources are available
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

def clean_text(text):
    """Clean text similarly to your original implementation."""
    if isinstance(text, float):
        return ""
    text = text.lower()
    printable = set(string.printable)
    return "".join(ch for ch in text if ch in printable)

def find_keywords(text):
    """
    Extract keywords using a PageRank-like approach over a co-occurrence matrix.
    1. The co-occurrence matrix is built as a sparse matrix.
    2. The PageRank update is fully vectorized.
    """
    # --- 1. Clean and tokenize ---
    cleaned_text = clean_text(text)
    tokens = word_tokenize(cleaned_text)
    if not tokens:
        return ""
    
    # --- 2. POS tagging and lemmatization ---
    pos_tags = nltk.pos_tag(tokens)
    lemmatizer = WordNetLemmatizer()
    adjective_tags = {"JJ", "JJR", "JJS"}
    lemmatized_text = [
        lemmatizer.lemmatize(word, "a") if tag in adjective_tags else lemmatizer.lemmatize(word)
        for word, tag in pos_tags
    ]
    pos_tags_2 = nltk.pos_tag(lemmatized_text)
    
    # Build stopword list based on POS filtering: keep only certain tags
    keep_tags = {"NN", "NNS", "NNP", "NNPS", "JJ", "JJR", "JJS", "VBG", "FW"}
    removed_words = [w for (w, t) in pos_tags_2 if t not in keep_tags]
    
    # Also use punctuation and an optional file of extra stopwords
    punctuation = list(string.punctuation)
    additional_stopwords = []
    try:
        with open("long_stopwords.txt", "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    additional_stopwords.append(line)
    except FileNotFoundError:
        pass
    stopwords_plus = set(removed_words + punctuation + additional_stopwords)
    
    # Filter out the stopwords
    processed_text = [w for w in lemmatized_text if w not in stopwords_plus]
    if not processed_text:
        return ""
    
    # --- 3. Build the sparse co-occurrence (adjacency) matrix ---
    vocabulary = list(set(processed_text))
    vocab_len = len(vocabulary)
    word_index = {word: idx for idx, word in enumerate(vocabulary)}
    window_size = 3
    
    # Use a sparse matrix for efficiency
    weighted_edge = lil_matrix((vocab_len, vocab_len), dtype=np.float32)
    
    # Build edges over each sliding window
    for start_i in range(len(processed_text) - window_size + 1):
        window_words = processed_text[start_i : start_i + window_size]
        for j in range(window_size):
            idx_i = word_index[window_words[j]]
            for k in range(j + 1, window_size):
                idx_j = word_index[window_words[k]]
                if idx_i == idx_j:
                    continue
                # Use distance-based weighting: 1/distance
                dist = abs(j - k) or 1
                weighted_edge[idx_i, idx_j] += 1.0 / dist
                weighted_edge[idx_j, idx_i] += 1.0 / dist

    # --- 4. Vectorized PageRank Computation ---
    weighted_edge = weighted_edge.tocsr()
    
    # Sum of weights per node
    inout = np.array(weighted_edge.sum(axis=1)).flatten()
    inout[inout == 0] = 1e-9  # prevent division by zero

    # Normalize columns to build the transition matrix T
    T_csc = weighted_edge.tocsc()
    for j in range(vocab_len):
        start_ptr, end_ptr = T_csc.indptr[j], T_csc.indptr[j+1]
        if inout[j] != 0:
            T_csc.data[start_ptr:end_ptr] /= inout[j]
    T = T_csc.tocsr()
    
    # PageRank parameters
    damping_factor = 0.85
    max_iterations = 50
    threshold = 1e-4
    score = np.ones(vocab_len, dtype=np.float32)
    
    # Vectorized PageRank update
    for _ in range(max_iterations):
        prev_score = score.copy()
        score = (1 - damping_factor) + damping_factor * (T.dot(prev_score))
        if np.sum(np.abs(prev_score - score)) <= threshold:
            break

    # --- 5. Build phrases ---
    phrases = []
    current_phrase = []
    for w in lemmatized_text:
        if w in stopwords_plus:
            if current_phrase:
                phrases.append(current_phrase)
                current_phrase = []
        else:
            current_phrase.append(w)
    if current_phrase:
        phrases.append(current_phrase)
    
    # Remove duplicate phrases while preserving order
    unique_phrases = []
    seen = set()
    for ph in phrases:
        tup = tuple(ph)
        if tup not in seen:
            seen.add(tup)
            unique_phrases.append(ph)
    
    # Remove single-word phrases if they appear within any multi-word phrase
    for w in vocabulary:
        single = [w]
        if single in unique_phrases:
            for ph in unique_phrases:
                if len(ph) > 1 and w in ph:
                    unique_phrases.remove(single)
                    break

    # --- 6. Score phrases and select the top keywords ---
    phrase_scores = []
    keywords = []
    for ph in unique_phrases:
        p_score = sum(score[word_index[w]] for w in ph if w in word_index)
        phrase_scores.append(p_score)
        keywords.append(" ".join(ph))
    
    sorted_idx = np.argsort(phrase_scores)[::-1]
    keywords_num = min(50, len(sorted_idx))
    top_keywords = [keywords[i] for i in sorted_idx[:keywords_num]]
    
    return ", ".join(top_keywords)

def topic_modeling_lda(documents, n_topics=8, n_top_words=10):
    """
    Perform LDA-based topic modeling.
    Each row in 'documents' is treated as a separate document.
    Prints out the top n_top_words for each topic.
    """
    # Convert documents to a matrix of token counts
    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)
    doc_term_matrix = vectorizer.fit_transform(documents)
    
    # Fit LDA
    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda_model.fit(doc_term_matrix)
    
    feature_names = vectorizer.get_feature_names_out()
    
    print("\nLDA Topics:")
    for topic_idx, topic in enumerate(lda_model.components_):
        # Sort by descending weight
        top_indices = topic.argsort()[::-1][:n_top_words]
        top_features = [feature_names[i] for i in top_indices]
        print(f"Topic {topic_idx+1}: {', '.join(top_features)}")

def topic_modeling_nmf(documents, n_topics=8, n_top_words=10):
    """
    Perform NMF-based topic modeling.
    Each row in 'documents' is treated as a separate document.
    Prints out the top n_top_words for each topic.
    """
    # Convert documents to a TF-IDF matrix
    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)
    tfidf_matrix = vectorizer.fit_transform(documents)
    
    # Fit NMF
    nmf_model = NMF(n_components=n_topics, random_state=42)
    nmf_model.fit(tfidf_matrix)
    
    feature_names = vectorizer.get_feature_names_out()
    
    print("\nNMF Topics:")
    for topic_idx, topic in enumerate(nmf_model.components_):
        # Sort by descending weight
        top_indices = topic.argsort()[::-1][:n_top_words]
        top_features = [feature_names[i] for i in top_indices]
        print(f"Topic {topic_idx+1}: {', '.join(top_features)}")

def main():
    # Load the Excel file and target column (adjust path/column as needed)
    df = pd.read_excel(r"C:/Users/2025344/Demo1/issues_data.xlsx")
    # Suppose we're interested in column index 13 (0-based indexing, i.e., 14th column):
    column_data = df.iloc[:, 13]
    
    # Convert each row into a single document for topic modeling
    documents = column_data.dropna().astype(str).tolist()
    
    # 1. Use your existing keyword extraction on the AGGREGATED text (80% of it)
    combined_text = " ".join(documents)
    split_index = int(len(combined_text) * 0.8)
    train_text = combined_text[:split_index]
    keywords = find_keywords(train_text)
    print("Top keywords/phrases (PageRank-based):")
    print(keywords)
    
    # 2. LDA topic modeling
    topic_modeling_lda(documents, n_topics=8, n_top_words=10)
    
    # 3. NMF topic modeling
    topic_modeling_nmf(documents, n_topics=8, n_top_words=10)

if __name__ == "__main__":
    main()
